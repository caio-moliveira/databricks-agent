# üåê Consumindo Modelos Databricks com o OpenAI Client

Este projeto demonstra como interagir com um **LLM Endpoint** (Modelo Servido) no Databricks usando o cliente Python da OpenAI. Essa abordagem √© poss√≠vel porque o Databricks Model Serving adota a mesma especifica√ß√£o de API de *completion* de chat que a OpenAI.

## üó∫Ô∏è Roadmap de Configura√ß√£o

### 1\. Prepara√ß√£o do Databricks LLM Endpoint

Antes de executar o c√≥digo, voc√™ deve ter um LLM servido no seu Databricks Workspace:

1.  **Modelo Servido (LLM Endpoint):** Certifique-se de que o modelo que voc√™ deseja usar (ex: `databricks-meta-llama-3-1-405b-instruct`) est√° configurado e ativo em **Model Serving** no Databricks.
2.  **API URL (Base URL):** O endpoint que voc√™ usar√° como `base_url` no cliente OpenAI. O formato t√≠pico √©:
    ```
    https://<DATABRICKS_HOST>/serving-endpoints
    ```
    *Para obter o endere√ßo correto, acesse a p√°gina de detalhes do seu Endpoint de LLM no Databricks.*

### 2\. Configura√ß√£o de Vari√°veis de Ambiente

Crie um arquivo `.env` para armazenar as credenciais necess√°rias.

| Vari√°vel | Descri√ß√£o | Exemplo de Valor |
| :--- | :--- | :--- |
| `DATABRICKS_TOKEN` | Seu Token de Acesso Pessoal (PAT) do Databricks. **Este token ser√° usado como `api_key` no cliente OpenAI.** | `dapi...` |
| `base_url` | O **Base URL** do seu LLM Endpoint (API URL). | `https://adb-XXX.cloud.databricks.com/serving-endpoints` |

### 3\. Instala√ß√£o de Depend√™ncias

Voc√™ precisar√° apenas do cliente `openai` e do `python-dotenv`:

```bash
pip install openai python-dotenv
```

-----

## üíª Explica√ß√£o do Arquivo `serving-models-databricks.py`

O script demonstra como o cliente OpenAI √© configurado para apontar para o Databricks em vez do servi√ßo original da OpenAI.

### 1\. Configura√ß√£o da Conex√£o

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

client = OpenAI(
    api_key=os.getenv("DATABRICKS_TOKEN"),
    base_url=os.getenv("base_url"),
)
```

| Par√¢metro no `OpenAI(client=...)` | Valor Atribu√≠do | Fun√ß√£o |
| :--- | :--- | :--- |
| `api_key` | **`DATABRICKS_TOKEN`** | O Databricks utiliza o **Token de Acesso Pessoal (PAT)** como chave de autentica√ß√£o (API Key) para acessar o servi√ßo de Model Serving. |
| `base_url` | **`base_url`** | O endere√ßo da API do Databricks (Ex: `https://.../serving-endpoints`). Isso redireciona todas as chamadas do cliente OpenAI para o seu ambiente Databricks. |

### 2\. Invoca√ß√£o do Modelo (Chat Completion)

A chamada para `client.chat.completions.create` √© padr√£o da OpenAI, mas com algumas modifica√ß√µes cruciais.

```python
response = client.chat.completions.create(
    model="databricks-meta-llama-3-1-405b-instruct",
    messages=[{"role": "user", "content": "What is Databricks?"}],
    temperature=0,
    extra_body={"usage_context": {"project": "project1"}},
)
```

| Par√¢metro | Fun√ß√£o e Detalhe |
| :--- | :--- |
| `model` | Deve ser o **nome exato** do seu LLM Endpoint configurado no Databricks. |
| `messages` | O formato de entrada de chat padr√£o (lista de dicion√°rios com `role` e `content`). |
| `temperature` | Controla a aleatoriedade da resposta (0 = determin√≠stica, 1 = criativa). |
| `extra_body` | **(Opcional, mas √∫til no Databricks)** Permite passar metadados, como `usage_context`, para fins de rastreamento de custos e monitoramento no seu Workspace. |

### 3\. Execu√ß√£o

Ao rodar o script, o cliente OpenAI enviar√° a requisi√ß√£o para o `base_url` (Databricks) usando o token como chave, e o Databricks retornar√° a resposta gerada pelo LLM.

```python
# Execu√ß√£o da resposta
answer = response.choices[0].message.content
print("Answer:", answer)
```

Este m√©todo simplifica a integra√ß√£o, permitindo que voc√™ utilize a vasta documenta√ß√£o e bibliotecas do ecossistema OpenAI, enquanto executa os modelos no seu ambiente Databricks seguro e escal√°vel.