Aqui estÃ¡ uma **versÃ£o revisada, mais clara, mais atrativa e mais organizada** da sua introduÃ§Ã£o â€” mantendo **todas as imagens, badges, links e estrutura**, mas elevando o texto para um nÃ­vel mais profissional e convidativo:

---

<div align="center">
  <img src="./assets/jornada.png" alt="Jornada de Dados" width="200"/>

# **Workshop: Databricks â€“ Construindo Agentes de IA**

### [Jornada de Dados](https://suajornadadedados.com.br/)

**Workshop prÃ¡tico sobre criaÃ§Ã£o de agentes de IA no Databricks, integrando LangChain e Vector Search**

[![Workshop](https://img.shields.io/badge/Workshop-Agentes%20de%20IA-blue?style=for-the-badge)](https://jornadadedados.alpaclass.com/c/cursos/jAZX23)
[![Databricks](https://img.shields.io/badge/Databricks-Lakehouse-orange?style=for-the-badge&logo=databricks)](https://www.databricks.com/)
[![LangChain](https://img.shields.io/badge/LangChain-Framework-blueviolet?style=for-the-badge&logo=chainlink)](https://docs.langchain.com/)
[![Python](https://img.shields.io/badge/Python-3.12+-green?style=for-the-badge\&logo=python)](https://python.org)
[![OpenAI](https://img.shields.io/badge/OpenAI-API-lightgrey?style=for-the-badge\&logo=openai)](https://openai.com/)

</div>


## ğŸ¯ **Sobre o Workshop**

Este repositÃ³rio faz parte do conteÃºdo prÃ¡tico do **Workshop Databricks** da **Jornada de Dados**, onde desenvolvemos um projeto completo de **Agentes de IA corporativos** com foco em integraÃ§Ã£o entre **Databricks + LangChain**.

O objetivo Ã© mostrar, passo a passo, como construir uma soluÃ§Ã£o moderna de InteligÃªncia Artificial que combina:

* Vector Search para buscas semÃ¢nticas
* LLMs hospedados nativamente no Databricks
* LangChain para orquestrar a arquitetura RAG
* MLflow para rastreamento e observabilidade
* Streamlit como interface amigÃ¡vel para o usuÃ¡rio final

---

## ğŸ¤– **O que vocÃª vai construir**

Durante este mÃ³dulo, vocÃª desenvolverÃ¡ um **Chatbot Financeiro Inteligente**, capaz de:

* Consultar dados estruturados e semiestruturados
* Responder perguntas de negÃ³cio em linguagem natural
* Interpretar registros de vendas, clientes e produtos
* Utilizar Vector Search para respostas baseadas em similaridade
* Registrar toda a execuÃ§Ã£o da pipeline no MLflow
* Expor a soluÃ§Ã£o em um frontend interativo com Streamlit

Esse Ã© um **projeto completo**, replicÃ¡vel e pronto para uso em ambientes reais de negÃ³cio.

---

# ğŸ“ Estrutura Geral do Projeto

```
project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ clientes.csv
â”‚   â”œâ”€â”€ produtos.csv
â”‚   â””â”€â”€ vendas.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ settings.py
â””â”€â”€ README.md
```

---

# ğŸ§± 1. Criando o Catalog, Schema e Volume

No Databricks:

1. **Catalog:** `ai-agent-workshop`
2. **Schema:** `data`
3. **Volume:** crie um volume dentro do schema (Eu chamei o meu de 'data')
4. FaÃ§a upload dos arquivos CSV dentro de:

```
ai-agent-workshop / data / <seu-volume>
```

---

# ğŸ”¥ 2. Criando as Tabelas do Projeto

Abra um **notebook** e execute:

```sql
USE CATALOG `ai-agent-workshop`;
```

### **Carregando CSVs**

```python
# Ajuste os paths conforme o nome do volume criado no Databricks
path_clientes = "dbfs:/Volumes/ai-agent-workshop/data/data/clientes.csv"
path_produtos = "dbfs:/Volumes/ai-agent-workshop/data/data/produtos.csv"
path_vendas   = "dbfs:/Volumes/ai-agent-workshop/data/data/vendas.csv"

df_clientes = (
    spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .csv(path_clientes)
)

df_produtos = (
    spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .csv(path_produtos)
)

df_vendas = (
    spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .csv(path_vendas)
)
```

### **Criando as Tabelas Delta**

```python
df_clientes.write.mode("overwrite").saveAsTable("`ai-agent-workshop`.data.clientes")
df_produtos.write.mode("overwrite").saveAsTable("`ai-agent-workshop`.data.produtos")
df_vendas.write.mode("overwrite").saveAsTable("`ai-agent-workshop`.data.vendas")
```

---

# ğŸ“Š 3. Criando a View AnalÃ­tica

```sql
CREATE OR REPLACE VIEW `ai-agent-workshop`.data.vw_financas_vendas AS
SELECT
  v.id_venda,
  v.data_venda,
  DATE(v.data_venda)                AS data,
  YEAR(v.data_venda)                AS ano,
  MONTH(v.data_venda)               AS mes,

  -- Cliente
  v.id_cliente,
  c.nome_cliente,
  c.segmento        AS segmento_cliente,
  c.cidade          AS cidade_cliente,
  c.estado          AS estado_cliente,

  -- Produto
  v.id_produto,
  p.nome_produto,
  p.categoria       AS categoria_produto,

  -- MÃ©tricas da venda
  v.quantidade,
  v.valor_unitario,
  v.valor_total     AS receita_venda,
  v.canal_venda

FROM `ai-agent-workshop`.data.vendas   AS v
JOIN `ai-agent-workshop`.data.clientes AS c
  ON v.id_cliente = c.id_cliente
JOIN `ai-agent-workshop`.data.produtos AS p
  ON v.id_produto = p.id_produto;
```

---

# ğŸ§¬ 4. Tabela SemÃ¢ntica para RAG

```sql
CREATE OR REPLACE TABLE `ai-agent-workshop`.data.financas_semantica AS
SELECT
  CAST(id_venda AS STRING)                  AS id_registro,
  'venda'                                   AS tipo,
  CONCAT(
    'Venda ', CAST(id_venda AS STRING),
    ' realizada em ', CAST(data AS STRING),
    ' para o cliente ', nome_cliente,
    ' (segmento ', segmento_cliente, ', cidade ', cidade_cliente, ' - ', estado_cliente, '). ',
    'Produto: ', nome_produto, ' (categoria ', categoria_produto, '). ',
    'Quantidade: ', CAST(quantidade AS STRING),
    ', valor unitÃ¡rio R$', CAST(valor_unitario AS STRING),
    ', valor total R$', CAST(receita_venda AS STRING),
    '. Canal de venda: ', canal_venda, '.'
  ) AS texto_busca,

  -- Metadados adicionais Ãºteis
  data,
  ano,
  mes,
  nome_cliente,
  segmento_cliente,
  cidade_cliente,
  estado_cliente,
  nome_produto,
  categoria_produto,
  quantidade,
  valor_unitario,
  receita_venda,
  canal_venda
FROM `ai-agent-workshop`.data.vw_financas_vendas;
```

---

# ğŸ§­ 5. Criando a Tabela Vetorial

Navegue no Databricks:

1. **Compute â†’ Vector Search**
2. Clique em **Create Endpoint**
3. Nome: `my-vector-search`
4. Associe Ã  tabela:

```
ai-agent-workshop.data.financas_semantica
```

5. Crie o Ã­ndice:

```
ai-agent-workshop.data.financas_semantica_index
```


---

# â–¶ï¸ 6. InstalaÃ§Ã£o & ExecuÃ§Ã£o

> PrÃ©-requisitos: **Python 3.13+** e **uv** instalado (`pip install uv` ou veja a doc do uv).

### 1) Clonar o repositÃ³rio

```bash
git clone https://github.com/<seu-usuario>/<seu-repo>.git
cd <seu-repo>
```

### 2) Criar e ativar o ambiente com `uv`

```bash
# (Opcional) garantir Python 3.12 disponÃ­vel pelo uv
uv python install 3.13

# criar venv
uv venv
```

**Ativar o ambiente:**

* **Windows (PowerShell):**

```powershell
. .venv\Scripts\Activate
```

* **macOS / Linux:**

```bash
source .venv/bin/activate
```

### 3) Instalar dependÃªncias do `pyproject.toml`

```bash
uv sync
```

> O `uv sync` instala tudo que estÃ¡ no `pyproject.toml` (e `uv.lock`, se existir), sem precisar de `requirements.txt`.

### 4) Configurar variÃ¡veis de ambiente (`.env`)

Crie um arquivo `.env` na raiz do projeto com as variÃ¡veis abaixo (ajuste os valores):

```env
# === Credenciais & Acesso ===
OPENAI_API_KEY=sk-xxxxxx                                # se usar OpenAI (opcional conforme seu LLM)
DATABRICKS_HOST=https://<seu-workspace>.cloud.databricks.com
DATABRICKS_TOKEN=dapi-xxxxxxxxxxxxxxxxxxxxxxxx

# === MLflow ===
MLFLOW_TRACKING_URI=databricks                          # mantÃ©m "databricks"
EXPERIMENT_ID=1234567890                                # ID do experimento no Databricks

# === Vector Search ===
VS_ENDPOINT=my-vector-search                            # nome do endpoint criado em Compute > Vector Search
VS_INDEX=ai-agent-workshop.data.financas_semantica_index

# === LLM no Databricks ===
LLM_EP=databricks-meta-llama-3-3-70b-instruct           # endpoint de modelo (ajuste para o seu)
MODEL_NAME=langchain_rag_demo                           # identificador lÃ³gico interno (livre)

# === (Opcional) Conta/Workspace ===
DATABRICKS_ACCOUNT_ID=                                  # use se necessÃ¡rio em sua org
```

> O projeto jÃ¡ carrega essas variÃ¡veis via `dotenv` no `settings.py`.

### 5) Executar o app (Streamlit)

```bash
streamlit run app.py
```

Acesse: `http://localhost:8501`

---
